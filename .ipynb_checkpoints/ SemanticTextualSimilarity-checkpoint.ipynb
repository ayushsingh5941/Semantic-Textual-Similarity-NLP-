{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import names, stopwords\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Text_Similarity_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Familiarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>india seeks to boost construction india has cl...</td>\n",
       "      <td>music mogul fuller sells company pop idol supr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>podcasters  look to net money nasa is doing it...</td>\n",
       "      <td>ukip outspent labour on eu poll the uk indepen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>row over  police  power for csos the police fe...</td>\n",
       "      <td>ban on hunting comes into force fox hunting wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>election  could be terror target  terrorists m...</td>\n",
       "      <td>nhs waiting time target is cut hospital waitin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>japan economy slides to recession the japanese...</td>\n",
       "      <td>optimism remains over uk housing the uk proper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  savvy searchers fail to spot ads internet sear...   \n",
       "1          1  millions to miss out on the net by 2025  40% o...   \n",
       "2          2  young debut cut short by ginepri fifteen-year-...   \n",
       "3          3  diageo to buy us wine firm diageo  the world s...   \n",
       "4          4  be careful how you code a new european directi...   \n",
       "5          5  india seeks to boost construction india has cl...   \n",
       "6          6  podcasters  look to net money nasa is doing it...   \n",
       "7          7  row over  police  power for csos the police fe...   \n",
       "8          8  election  could be terror target  terrorists m...   \n",
       "9          9  japan economy slides to recession the japanese...   \n",
       "\n",
       "                                               text2  \n",
       "0  newcastle 2-1 bolton kieron dyer smashed home ...  \n",
       "1  nasdaq planning $100m share sale the owner of ...  \n",
       "2  ruddock backs yapp s credentials wales coach m...  \n",
       "3  mci shares climb on takeover bid shares in us ...  \n",
       "4  media gadgets get moving pocket-sized devices ...  \n",
       "5  music mogul fuller sells company pop idol supr...  \n",
       "6  ukip outspent labour on eu poll the uk indepen...  \n",
       "7  ban on hunting comes into force fox hunting wi...  \n",
       "8  nhs waiting time target is cut hospital waitin...  \n",
       "9  optimism remains over uk housing the uk proper...  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4023 entries, 0 to 4022\n",
      "Data columns (total 3 columns):\n",
      "Unique_ID    4023 non-null int64\n",
      "text1        4023 non-null object\n",
      "text2        4023 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 94.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique_ID    0\n",
       "text1        0\n",
       "text2        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data (4023, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Size of data', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in document 3383889\n"
     ]
    }
   ],
   "source": [
    "text_size1 = data['text1'].apply(lambda x: len(x.split(' '))).sum()\n",
    "text_size2 = data['text2'].apply(lambda x: len(x.split(' '))).sum()\n",
    "print('Total number of words in document', text_size1+text_size2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are total 4023 rows and 3 columns. columns are Unique_ID, text1 and text2. we will drop Unique_ID column as it is not relevant to us or we can simply make Unique_ID as index column for data.\n",
    "This data set do not have any missing values. Total 33,83,889 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_ID = data['Unique_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Unique_ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing stopwords, all names, English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = set(names.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_string(string):\n",
    "    \"\"\"\n",
    "    Function to clean each string of dataset\n",
    "    \"\"\"\n",
    "    # Removing all the digits\n",
    "    string = re.sub(r'\\d', '', string)\n",
    "    \n",
    "    # Removing Mentions\n",
    "    string = re.sub(r'@\\w+', ' ', string)\n",
    "    \n",
    "    # Removing links \n",
    "    string = re.sub(r'(https?:\\/\\/)?([\\da-zA-Z\\.-\\/\\#\\:]+)\\.([\\da-zA-Z\\.\\/\\:\\#]{0,9})([\\/\\w \\.-\\/\\:\\#]*)', ' ', string)\n",
    "    \n",
    "    # Removing all the digits special caharacters\n",
    "    string = re.sub(r'\\W', ' ', string)\n",
    "        \n",
    "  \n",
    "   \n",
    "    string = string.strip()\n",
    "    \n",
    "    #Removing all Single characters\n",
    "    string = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", string)\n",
    "    \n",
    "    \n",
    "    # Lemmetizing the string and removing stop words\n",
    "    string = string.split()\n",
    "    string = [lemma.lemmatize(word) for word in string if word not in stop_words]\n",
    "    string = ' '.join(string)\n",
    "    \n",
    "    # Lowercasing all data\n",
    "    string = string.lower()\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    \"\"\"\n",
    "    Function to retrieve each value in dataset and pass it through cleaned_string function.\n",
    "    \"\"\"\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            data.iloc[i, j] = cleaned_string(data.iloc[i, j])\n",
    "    return data\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clean_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>savvy searcher fail spot ad internet search en...</td>\n",
       "      <td>newcastle bolton kieron dyer smashed home winn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>million miss net uk population still without i...</td>\n",
       "      <td>nasdaq planning share sale owner technology do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short ginepri fifteen year old...</td>\n",
       "      <td>ruddock back yapp credential wale coach mike r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>diageo buy wine firm diageo world biggest spir...</td>\n",
       "      <td>mci share climb takeover bid share phone compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>careful code new european directive could put ...</td>\n",
       "      <td>medium gadget get moving pocket sized device l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  savvy searcher fail spot ad internet search en...   \n",
       "1  million miss net uk population still without i...   \n",
       "2  young debut cut short ginepri fifteen year old...   \n",
       "3  diageo buy wine firm diageo world biggest spir...   \n",
       "4  careful code new european directive could put ...   \n",
       "\n",
       "                                               text2  \n",
       "0  newcastle bolton kieron dyer smashed home winn...  \n",
       "1  nasdaq planning share sale owner technology do...  \n",
       "2  ruddock back yapp credential wale coach mike r...  \n",
       "3  mci share climb takeover bid share phone compa...  \n",
       "4  medium gadget get moving pocket sized device l...  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in document after preocessing 448910\n"
     ]
    }
   ],
   "source": [
    "text_size1 = x['text1'].apply(lambda x: len(x.split(' '))).sum()\n",
    "text_size2 = x['text2'].apply(lambda x: len(x.split(' '))).sum()\n",
    "print('Total number of words in document after preocessing', text_size1+text_size2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively small dataset with only 4,48,910 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data in array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splited in array to label it because doc2vec requires labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text_list, texts, text_list_name):\n",
    "# function to transform questions and display progress\n",
    "    for text in texts:\n",
    "        text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = []\n",
    "process_text(text1, x['text1'], 'text1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = []\n",
    "process_text(text2, x['text2'], 'text2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labeled = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text1)):\n",
    "    # Text strings need to be separated into words\n",
    "    # Each Text string needs a unique label\n",
    "    text_labeled.append(TaggedDocument(text1[i].split( ), data[data.index ==i]))\n",
    "    text_labeled.append(TaggedDocument(text2[i].split( ), data[data.index ==i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm = 1, min_count=1, window=2, vector_size=300, sample=1e-4, negative=10, workers=1, seed=42)\n",
    "model.build_vocab(text_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 is complete.\n",
      "Epoch #2 is complete.\n",
      "Epoch #3 is complete.\n",
      "Epoch #4 is complete.\n",
      "Epoch #5 is complete.\n",
      "Epoch #6 is complete.\n",
      "Epoch #7 is complete.\n",
      "Epoch #8 is complete.\n",
      "Epoch #9 is complete.\n",
      "Epoch #10 is complete.\n",
      "Epoch #11 is complete.\n",
      "Epoch #12 is complete.\n",
      "Epoch #13 is complete.\n",
      "Epoch #14 is complete.\n",
      "Epoch #15 is complete.\n",
      "Epoch #16 is complete.\n",
      "Epoch #17 is complete.\n",
      "Epoch #18 is complete.\n",
      "Epoch #19 is complete.\n",
      "Epoch #20 is complete.\n",
      "Epoch #21 is complete.\n",
      "Epoch #22 is complete.\n",
      "Epoch #23 is complete.\n",
      "Epoch #24 is complete.\n",
      "Epoch #25 is complete.\n",
      "Epoch #26 is complete.\n",
      "Epoch #27 is complete.\n",
      "Epoch #28 is complete.\n",
      "Epoch #29 is complete.\n",
      "Epoch #30 is complete.\n",
      "Epoch #31 is complete.\n",
      "Epoch #32 is complete.\n",
      "Epoch #33 is complete.\n",
      "Epoch #34 is complete.\n",
      "Epoch #35 is complete.\n",
      "Epoch #36 is complete.\n",
      "Epoch #37 is complete.\n",
      "Epoch #38 is complete.\n",
      "Epoch #39 is complete.\n",
      "Epoch #40 is complete.\n",
      "Epoch #41 is complete.\n",
      "Epoch #42 is complete.\n",
      "Epoch #43 is complete.\n",
      "Epoch #44 is complete.\n",
      "Epoch #45 is complete.\n",
      "Epoch #46 is complete.\n",
      "Epoch #47 is complete.\n",
      "Epoch #48 is complete.\n",
      "Epoch #49 is complete.\n",
      "Epoch #50 is complete.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model.train(text_labeled,epochs=model.epochs,total_examples=model.corpus_count)\n",
    "    print(\"Epoch #{} is complete.\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of vocabulary words in model: 14123\n"
     ]
    }
   ],
   "source": [
    "print('No. of vocabulary words in model:', len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting each word in a row to match vocab during model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_split = []\n",
    "for text in text1:\n",
    "    text1_split.append(text.split())\n",
    "    \n",
    "text2_split = []\n",
    "for text in text2:\n",
    "    text2_split.append(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text1_split)):\n",
    "    score = model.wv.n_similarity(text1_split[i], text2_split[i])\n",
    "    doc2vec_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = sum(doc2vec_scores) / len(doc2vec_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score 0.7841522685188931\n"
     ]
    }
   ],
   "source": [
    "print('Mean score', mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Unique_ID': Unique_ID,\n",
    "                       'Similarity_Score': doc2vec_scores})\n",
    "\n",
    "output.to_csv(r'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean score 0.7841522685188931a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
